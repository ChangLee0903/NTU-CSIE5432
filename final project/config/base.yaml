base:
  target_drop_list:
    [
      "arrival_date_year",
      "net_cancelled",
      "Room",
      "ID",
      "reservation_status_date",
      "reservation_status",
    ]
  cancel_drop_list:
    [
      "company",
      "arrival_date_year",
      "ID",
      "reservation_status_date",
      "reservation_status",
    ]
  default:
    [
      "arrival_date_year",
      "arrival_date_day_of_month",
      "ID",
      "reservation_status_date",
      "reservation_status",
    ]

RFR:
  n_jobs: 12
  criterion: "mae"
  n_estimators: 200
  verbose: False

CBR:
  loss_function: "MAE"
  learning_rate: 0.1
  iterations: 10000
  task_type: "GPU"
  devices: cuda:0


HGBR:
  # loss{‘least_squares’, ‘least_absolute_deviation’, ‘poisson’}, default=’least_squares’
  loss: "least_absolute_deviation"
  learning_rate: 0.1
  max_iter: 3000
  # max_leaf_nodes: 64
  verbose: False

GBR:
  # loss{‘least_squares’, ‘least_absolute_deviation’, ‘poisson’}, default=’least_squares’
  loss: "lad"
  criterion: "mae"
  learning_rate: 0.1
  verbose: False
  n_estimators: 100

RFC:
  n_jobs: 6
  # “gini”, “entropy”
  criterion: "entropy"
  # class_weight: { 1: 1, 0: 50 }
  n_estimators: 150
  verbose: False

XGBC:
  # parameter usage is here [https://xgboost.readthedocs.io/en/stable/parameter.html]
  # and here [https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn]
  # and here [https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst]
  booster: 'gbtree'
  eval_metric: 'mae'
  n_estimators: 200
  max_depth: 16
  learning_rate: 0.05
  objective: "binary:logistic"
  use_label_encoder: False
  nthread: 12
  # verbosity: 3
  # random_state: 1126
  # Control the balance of positive and negative weights, useful for unbalanced classes.
  # A typical value to consider: sum(negative instances) / sum(positive instances).
  # scale_pos_weight: 2

CBC:
  n_estimators: 1000
  class_weights: [5, 1]
  # depth: 16
  # learning_rate: 0.1
  # task_type: "GPU"
  # devices: cuda:0

HGBC:
  loss: 'binary_crossentropy'
  learning_rate: 0.5
  max_iter: 500
  max_leaf_nodes: 28

SVR:
  # kernel{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, default=’rbf’
  kernel: "rbf"
  # gamma{‘scale’, ‘auto’} or float, default=’scale’
  gamma: "scale"
  # tolerance: float, default=1e-3 Tolerance for stopping criterion.
  tol: 0.001
  C: 10000
  # Epsilon in the epsilon-SVR model. It specifies the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value.
  epsilon: 0.1

DNR:
  hidden_layer: [1024, 512, 256, 128, 64]
  # dropout_list: [0.2, 0.1, 0.1, 0, 0]
  use_emb: False
  batch_size: 64
  total_step: 150000
  opt: Adam
  lr: 0.001
  n_jobs: 6
  activation: LeakyReLU
  device: cuda:1  

OC:
  batch_size: 64
  # total_step: 950000
  total_step: 5000
  opt: Adam
  lr: 0.001
  n_jobs: 6
  activation: LeakyReLU
  use_label: False
  device: cuda:1

LSTM:
  hidden_size: 256
  num_layers: 3
  bidirectional: False
GRU:
  hidden_size: 256
  num_layers: 3
  bidirectional: False
dataloader:
  train_batch_size: 6 # training batch size, 12 for pre-train, 6 for cpc exp
  eval_batch_size: 12
hyper:
  learning_rate: 2e-4 # Learning rate for opt: ['4e-3' for fine-tune, '4e-3' for regualr downstream task training]
  warmup_proportion: 0.07 # Proportion of training to perform linear rate warmup.
  gradient_clipping: 1.0 # Maximum gradient norm
  total_step: 500000 # total steps for training, a step is a batch of update
